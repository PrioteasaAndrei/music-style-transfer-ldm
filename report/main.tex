\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}

% Code listing settings
\lstset{
    language=Python,
    basicstyle=\ttfamily\tiny,
    breaklines=true,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!60!black},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    frame=single,
    showstringspaces=false
}

\title{Musical Style Transfer Using Latent Diffusion Models}
\author{Andrei Prioteasa (Matr. No.: 4740844) \and Theo Stempel-Hauburger (Matr. No.: 4740729)}
\date{\today}

\newcommand{\lecturename}{Lecture: Generative Neural Networks for the Sciences, WS 2024/25}
\newcommand{\lecturer}{Lecturer: Prof. Dr. Ullrich KÃ¶the}
\newcommand{\github}{GitHub Repository: \url{https://github.com/PrioteasaAndrei/music-style-transfer-ldm.git}}

\begin{document}

\maketitle
\begin{center}
    \lecturename \\
    \lecturer \\
    \vspace{0.5cm}
    \github
\end{center}

\tableofcontents
\begin{abstract}
    \textit{Andrei P.}\\
    \noindent In this project we implement a version of the approach proposed in the paper `Music Style Transfer With Diffusion Model'~\cite{huang2024music}. We experiment with the use of latent diffusion models for musical style transfer, working on grayscale mel-spectrograms (which are a visual representation of audio). We design a Latent Diffusion Model that can transfer the style of one instrument to another, while preserving the original tonal characteristics of the instrument. We collect and preprocess our own dataset from youtube videos, which we make available to the public (see \url{https://github.com/PrioteasaAndrei/music-style-transfer-ldm.git}). 
    
    We analyze the model capability of transferring the style of one instrument to another, generating new spectrograms conditioned on a style image and we run experiments on the architecture choices. We conclude that we are able to obtain blurry reconstruction, limited style transfer capabilities. We consider that the limited results of our model are due to the limited computational resources available to us and the complexity of the task, further training and refinement of our code is required to improve the results. We present a sample of our conditioned generation result at: \url{https://github.com/PrioteasaAndrei/music-style-transfer-ldm/blob/main/report/figures/content_aware_generated_audio_200ep.mp3}
   
\end{abstract}

\input{introduction}
\input{methodology}
\input{data}
\input{methods}
\input{results}
\input{conclusion}

\bibliographystyle{plain}
\bibliography{references}

\end{document} 