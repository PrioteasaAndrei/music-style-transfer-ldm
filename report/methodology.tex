\section{Methodology}

In this section, we will describe the methodology used to implement the proposed method. We will describe our approach to the architecture, the main components and the training process.

\subsection{Architecture Overview}
We tried to stay as close to the original paper as possible in terms of architecture, but the paper did not provide a detailed description. The main components of our model are:

\begin{table}[h]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Component} & \textbf{Description} \\
\hline
Spectrogram Encoder & Compresses the input spectrogram into a latent space \\
\hline
Style Encoder & Processes style spectrograms to extract multi-resolution style embeddings \\
\hline
Forward Diffusion & Implements the noise scheduler \\
\hline
UNet & Denoises the latent representation \\
\hline
Spectrogram Decoder & Reconstructs the final spectrogram from the latent space \\
\hline
DDIM & Reverse sampling process for generating new samples \\
\hline
Cross-Attention & Adds style information to the denoising process \\
\hline
VGGishFeatureLoss & Pretrained VGGish model to extract features from the spectrogram \\
\hline
\end{tabular}
\caption{Main components of the model architecture}
\label{tab:model-components}
\end{table}

\noindent We will now briefly describe each of the components.

\subsubsection{Spectrogram Encoder and decoder}
The encoder compresses the input spectrogram into a latent space using a series of convolutional layers, which allows for unrestricted input size:
\begin{lstlisting}[basicstyle=\tiny]
class SpectrogramEncoder(nn.Module):
    def __init__(self, latent_dim=4):
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, latent_dim, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(latent_dim)
        )
\end{lstlisting}

\noindent  The decoder mirrors the encoder architecture but uses transposed convolutions to upsample back to the original dimensions, normalizing the output to be between -1 and 1:
\begin{lstlisting}[basicstyle=\tiny]
class SpectrogramDecoder(nn.Module):
    def __init__(self, latent_dim=4):
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Tanh()
        )
\end{lstlisting}

\noindent Both the encoder and decoder need to be pretrained on the spectrograms to be able to reconstruct the original audio. During the training process, we froze the encoder weights to prevent them from being updated, while leaving the decoder weights trainable. We describe this process in the experiments section.

\subsubsection{Style Encoder}
The style encoder processes style spectrograms to extract multi-resolution embeddings. Activation maps from different convolutional layers are extracted and used as conditioning mechianisms in the UNet, thorugh the Cross Attention mechanism.

\begin{lstlisting}[basicstyle=\tiny]
class StyleEncoder(nn.Module):
    def __init__(self, in_channels=1, num_filters=64):
        self.enc1 = nn.Conv2d(in_channels, num_filters, kernel_size=3, stride=2, padding=1)
        self.enc2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, stride=2, padding=1)
        self.enc3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, stride=2, padding=1)
        self.enc4 = nn.Conv2d(num_filters * 4, num_filters * 4, kernel_size=3, stride=2, padding=1)
        self.enc5 = nn.Conv2d(num_filters * 4, num_filters * 4, kernel_size=3, stride=2, padding=1)
        self.enc6 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=3, stride=2, padding=1)
\end{lstlisting}

\subsubsection{UNet}
The UNet is a standard denoising diffusion model, which is used to denoise the latent representation of the spectrogram. The UNet is conditioned on the style embeddings extracted by the style encoder using the Cross Attention mechanism. Skip connections are used to improve the training process. We use a sinusoidal position embedding to encode the time step in the UNet.


\newpage

\begin{lstlisting}[basicstyle=\tiny]
class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, num_filters=64):
        super(UNet, self).__init__()

        # Define the channel dimensions used in your UNet
        time_emb_dim = 128  # This should match the channel dimension where you add the time embedding
        
        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(time_emb_dim),  # Match the channel dimension
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.GELU(),
            nn.Linear(time_emb_dim, time_emb_dim),
        )

        # Downsampling path with proper padding to maintain spatial dimensions
        self.enc1 = nn.Conv2d(in_channels, num_filters, kernel_size=3, stride=1, padding=1)
        self.enc2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, stride=2, padding=1)  # 128x128
        self.enc3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, stride=2, padding=1)  # 64x64
        self.enc4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size=3, stride=2, padding=1)  # 32x32

        # Cross attention layers with correct embedding dimensions
        self.cross_attention1 = CrossAttention(embed_dim=512, num_heads=4)  # For 2x2 feature maps with 512 channels
        self.cross_attention2 = CrossAttention(embed_dim=256, num_heads=4)  # For 4x4 feature maps with 256 channels

        # Bottleneck
        self.bottleneck = nn.Conv2d(num_filters * 8, num_filters * 8, kernel_size=3, stride=1, padding=1)

        # Upsampling path with proper padding to maintain spatial dimensions
        self.dec4 = nn.ConvTranspose2d(num_filters * 8, num_filters * 4, kernel_size=3, stride=2, padding=1, output_padding=1)  # 64x64
        self.dec3 = nn.ConvTranspose2d(num_filters * 4, num_filters * 2, kernel_size=3, stride=2, padding=1, output_padding=1)  # 128x128
        self.dec2 = nn.ConvTranspose2d(num_filters * 2, num_filters, kernel_size=3, stride=2, padding=1, output_padding=1)  # 256x256
        self.dec1 = nn.Conv2d(num_filters, out_channels, kernel_size=3, stride=1, padding=1)

\end{lstlisting}

\subsubsection{ForwardDiffusion}

The forward diffusion process gradually adds Gaussian noise to the input data over a fixed number of timesteps. At each timestep $t$, the process is defined by:

\begin{equation}
    q(z_t|z_{t-1}) = \mathcal{N}(z_t; \sqrt{1-\beta_t}z_{t-1}, \beta_t\mathbf{I})
\end{equation}

\noindent where $\beta_t$ is a variance schedule that controls how much noise is added at each step. The process can be written in a closed form for any timestep $t$ as:

\begin{equation}
    q(z_t|z_0) = \mathcal{N}(z_t; \sqrt{\bar{\alpha}_t}z_0, (1-\bar{\alpha}_t)\mathbf{I})
\end{equation}

\noindent where $\alpha_t = 1-\beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. This allows us to sample $z_t$ directly for any timestep using the reparameterization trick:

\begin{equation}
    z_t = \sqrt{\bar{\alpha}_t}z_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I})
\end{equation}

The reverse process then learns to gradually denoise the data by predicting the noise $\epsilon$ at each timestep. Given a noisy sample $z_t$ and timestep $t$, we can predict the original input $z_0$ using:

\begin{equation}
    z_0 = \frac{z_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta(z_t,t)}{\sqrt{\bar{\alpha}_t}}
\end{equation}

\noindent where $\epsilon_\theta$ is our UNet model that predicts the noise. This formulation allows for stable training and high-quality generation.

\subsubsection{DDIM Sampling}
\label{sec:ddim_sampling}
% \begin{equation}
%     q(z_t|z_{t-1}) = \mathcal{N}(z_t; \sqrt{1-\beta_t}\,z_{t-1},\, \beta_t\mathbf{I}),
% \end{equation}
% and in closed form,
% \begin{equation}
%     q(z_t|z_0) = \mathcal{N}(z_t; \sqrt{\bar{\alpha}_t}\,z_0,\, (1-\bar{\alpha}_t)\mathbf{I}),
% \end{equation}
% with
% \begin{equation}
%     z_t = \sqrt{\bar{\alpha}_t}\,z_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I}).
% \end{equation}

This process was originally introduced by Song et al.~\cite{song2021denoising}. 
Opposed to the forward diffusion process, which gradually adds Gaussian noise to the input data, in DDIM sampling, our goal is to reverse this process. 
\\\\
Given the noisy latent representation of our original image \( z_t \), 
we use a trained UNet model \(\epsilon_\theta(z_t,t)\) to predict the noise in \( z_t \). 
From this prediction, we first estimate the original latent variable \( z_0 \) by rearranging the forward process equation:
\begin{equation}
    z_0^{\text{pred}} = \frac{z_t - \sqrt{1-\bar{\alpha}_t}\,\epsilon_\theta(z_t,t)}{\sqrt{\bar{\alpha}_t}}.
\end{equation}
When we have a lot of noise, this is a pretty hard task for the model and therefore the \( z_0^{\text{pred}} \) will only be a rough estimate.
Once we have this estimate, we add back a portion of the noise to reconstruct \( z_{t-1} \), the latent at the previous timestep.
In other words, we again “mix” the estimated \( z_0 \) with a portion of the noise to obtain a less noisy latent \( z_{t-1} \).
This update rule is given by:
\begin{equation}
    z_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\,z_0^{\text{pred}} + \sqrt{1-\bar{\alpha}_{t-1}}\,\epsilon_\theta(z_t,t)
\end{equation}
where \(\sqrt{\bar{\alpha}_{t-1}}\,z_0^{\text{pred}}\) represents the estimated latent at the previous timestep, and \(\sqrt{1-\bar{\alpha}_{t-1}}\,\epsilon_\theta(z_t,t)\) the corresponding noise at that timestep.  
\\[1ex]
This process is repeated iteratively, moving from the final noisy latent \( z_T \) down to \( z_0 \).
\\\\
In summary, while the forward process gradually introduces noise, the reverse DDIM update gradually removes it step by step. 
A good side effect of this approach is that each individual prediction by our model (if we only want to get new samples) does not need to be perfect.
Because as the noise level decreases in later stages, the task becomes easier, one can still generate good samples even if the model is not perfect at every step.


\subsubsection{Sinusoidal Position Embeddings}

The diffusion process requires knowledge of the timestep $t$ to properly denoise the data. However, neural networks work best with continuous representations rather than discrete timestep indices. Therefore, we use sinusoidal position embeddings to encode the timestep information in a way that the network can effectively utilize.

The sinusoidal encoding transforms a scalar timestep $t$ into a high-dimensional vector using sine and cosine functions at different frequencies:

\begin{equation}
    PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})
\end{equation}
\begin{equation}
    PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
\end{equation}

\noindent where $pos$ is the timestep and $i$ is the dimension. This encoding has several desirable properties:

\begin{itemize}
    \item It provides a unique encoding for each timestep
    \item The encoding varies smoothly with the timestep, allowing the model to interpolate between timesteps
    \item It captures both absolute and relative position information through the different frequency components
    \item The encoding is deterministic and requires no training
\end{itemize}


\noindent We employ the sinusoidal position embeddings to condition the model on the timestep. This is implemented as a multi-layer perceptron (MLP) that processes the timestep embedding:

\begin{lstlisting}[basicstyle=\tiny]
self.time_mlp = nn.Sequential(
    SinusoidalPositionEmbeddings(time_emb_dim),
    nn.Linear(time_emb_dim, time_emb_dim),
    nn.GELU(),
    nn.Linear(time_emb_dim, time_emb_dim),
)
\end{lstlisting}

\noindent The MLP first converts the scalar timestep into a high-dimensional embedding using sinusoidal position embeddings, then processes it through two linear layers with a GELU activation. This processed timestep embedding is then injected into multiple layers of the UNet to condition its denoising behavior on the specific timestep. This approach, originally introduced in the Transformer architecture \cite{vaswani2017attention}, has proven effective for encoding sequential position information in various deep learning applications, including diffusion models.


\subsubsection{VGGishFeatureLoss}
The VGGishFeatureLoss is a loss function that uses the pretrained VGGish\footnote{\url{https://github.com/harritaylor/torchvggish}} model to extract features from the spectrogram and the reconstructed spectrogram, and then computes the mean squared error at different resolutions between the two. 


\subsection{Training Process}

Our training objective is a weighted sum of a reconstruction loss, a style transfer loss and a diffusion loss. More formally, the training objective is:

\begin{equation}
    L = \lambda_{reconstruction}L_{reconstruction} + \lambda_{style}L_{style} + \lambda_{diffusion}L_{diffusion}
\end{equation}

\noindent Specifically, the reconstruction loss is defined as:

\begin{equation}
    \begin{split}
    L_{reconstruction}(x, \hat{x}, z) = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{x}_i)^2 + \\
    \lambda_{perceptual} \frac{1}{L}\sum_{l=1}^{L} MSE(\phi_l(x), \phi_l(\hat{x})) + \\
    \lambda_{kl} \frac{1}{2}\mathbb{E}[z^2 - 1 - \log(z^2 + \epsilon)]
    \end{split}
\end{equation}

\noindent where $\phi_l$ represents the feature maps at layer $l$ of the pretrained feature extractor network (VGGish or LPIPS). These feature maps capture increasingly abstract representations of the input spectrogram at different scales, from low-level features like edges in early layers to high-level semantic features in deeper layers.

\vspace{1em}

\noindent For the diffusion loss, we use the standard denoising diffusion loss which measures how well the model predicts noise at each timestep in the latent space:
% Diffusion Loss - measures how well the model predicts noise at each timestep
\begin{equation}
L_{diffusion}(\epsilon_\theta, \epsilon, t) = \frac{1}{n}\sum_{i=1}^{n}(\epsilon_{\theta,i}(z_t, t) - \epsilon_i)^2
\end{equation}

\noindent where $\epsilon_\theta$ is the predicted noise and $\epsilon$ is the true noise.

\vspace{1em}

\noindent For the style loss, we decide on measuring the MSE in the feature space of the pretrained feature extractor network (VGGish or LPIPS).

\begin{equation}
    L_{style}(x, \hat{x}, z) = 
    \frac{1}{L}\sum_{l=1}^{L} MSE(\phi_l(x), \phi_l(\hat{x}))
\end{equation}